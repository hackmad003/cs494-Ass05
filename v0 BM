#!/usr/bin/env python3
"""
PumpWatch GPU Benchmark

Compares CPU vs GPU performance for the anomaly detection pipeline.
Measures per-phase timing (feature extraction, training, inference)
and optionally sweeps batch sizes.

Outputs:
  - benchmarkResults.json  (structured timing data)
  - benchmarkReport.txt    (human-readable comparison table)
"""
import argparse
import json
import os
import sys
import time
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn

# Reuse model and data utilities from the main pipeline
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))
from detectAnomalies import (
    TinyAutoencoder,
    buildFeatureTable,
    setSeeds,
)


# ── Globals ──────────────────────────────────────────────────────────
DEFAULT_SEED = 7
DEFAULT_EPOCHS = 20
DEFAULT_BATCH_SIZE = 256
DEFAULT_LR = 1e-3
DEFAULT_WINDOW_SIZE = 50
DEFAULT_STRIDE = 10
DEFAULT_TRAIN_END_SEC = 330.0
DEFAULT_PERCENTILE = 99.5
DEFAULT_RUNS = 3


# ── Provided helper: prepare data ───────────────────────────────────
def prepareData(
    csvPath: str,
    windowSize: int = DEFAULT_WINDOW_SIZE,
    stride: int = DEFAULT_STRIDE,
    trainEndSec: float = DEFAULT_TRAIN_END_SEC,
) -> Tuple[np.ndarray, np.ndarray]:
    """Load CSV → build feature table → split into train/all normalized arrays."""
    df = pd.read_csv(csvPath)
    featDf = buildFeatureTable(df, windowSize, stride)
    featCols = ["f0", "f1", "f2", "f3", "f4", "f5"]

    trainMask = (featDf["tSec"] <= trainEndSec) & (featDf["isAnomaly"] == 0)
    trainFeat = featDf.loc[trainMask, featCols].to_numpy(dtype=np.float32)
    mu = trainFeat.mean(axis=0)

      sigma = trainFeat.std(axis=0) + 1e-6
    trainXn = (trainFeat - mu) / sigma
    allFeat = featDf[featCols].to_numpy(dtype=np.float32)
    allXn = (allFeat - mu) / sigma
    return trainXn, allXn


# ── Provided helper: train model on a given device ──────────────────
def trainOnDevice(
    trainX: np.ndarray,
    device: torch.device,
    epochs: int = DEFAULT_EPOCHS,
    batchSize: int = DEFAULT_BATCH_SIZE,
    lr: float = DEFAULT_LR,
) -> TinyAutoencoder:
    """Train TinyAutoencoder on the specified device. Returns trained model."""
    model = TinyAutoencoder(inputDim=trainX.shape[1], latentDim=8).to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    lossFn = nn.MSELoss()

    ds = torch.utils.data.TensorDataset(torch.from_numpy(trainX))
    dl = torch.utils.data.DataLoader(ds, batch_size=batchSize, shuffle=True)

    model.train()
    for epoch in range(epochs):
        for (xb,) in dl:
            xb = xb.to(device)
            opt.zero_grad(set_to_none=True)
            recon = model(xb)
            loss = lossFn(recon, xb)
            loss.backward()
            opt.step()
    return model


# ── Provided helper: inference on a given device ────────────────────
def inferOnDevice(
    model: TinyAutoencoder, x: np.ndarray, device: torch.device
) -> np.ndarray:
    """Run inference and return per-sample MSE."""
    model.eval()
    with torch.no_grad():
        xt = torch.from_numpy(x).to(device)
        recon = model(xt)
        mse = torch.mean((recon - xt) ** 2, dim=1).detach().cpu().numpy()
    return mse


# ────────────────────────────────────────────────────────────────────
# TODO 3a: timePhase() — Accurate GPU timing
#
# Write a function timePhase(func, device, warmup=True) that:
#   1. Optionally runs func() once as a warmup (discard the result timing)
#   2. If device is CUDA, calls torch.cuda.synchronize() BEFORE starting timer
#   3. Calls func() and captures its return value
#   4. If device is CUDA, calls torch.cuda.synchronize() AFTER func() returns
#   5. Uses time.perf_counter() for the timing
#   6. Returns (elapsedSeconds, returnValue)
#
#
# Why synchronize? GPU operations are asynchronous — without synchronize(),
# you'd measure the time to LAUNCH the GPU kernels, not the time for them
# to FINISH. This is the most common benchmarking mistake with CUDA.
# ────────────────────────────────────────────────────────────────────
def timePhase(func, device: torch.device, warmup: bool = True):
    """
    TODO 3a: Time a function with proper GPU synchronization.
    Returns (elapsedSeconds, returnValue).
    """
    # ── TODO 3a START ──
    print("    ⚠ TODO 3a: timePhase() is not yet implemented")
    t0 = time.perf_counter()
    result = func()
    elapsed = time.perf_counter() - t0
    return elapsed, result
    # ── TODO 3a END ──


# ────────────────────────────────────────────────────────────────────
# TODO 3b: benchmarkOneConfig() — Run full pipeline on one device
#
# Write a function benchmarkOneConfig(trainX, allX, device, cfg) that:
#   1. Resets seeds (setSeeds)
#   2. Uses timePhase() to time training (trainOnDevice)
#   3. Uses timePhase() to time inference (inferOnDevice on allX)
#   4. Returns a dict with keys:
#      {"device": str, "trainSec": float, "inferenceSec": float,
#       "totalSec": float, "threshold": float}
#
# The threshold is computed as np.percentile(trainMSE, percentile)
# where trainMSE comes from running inference on trainX.
# ────────────────────────────────────────────────────────────────────
def benchmarkOneConfig(
    trainX: np.ndarray,
    allX: np.ndarray,
    device: torch.device,
    batchSize: int = DEFAULT_BATCH_SIZE,
    epochs: int = DEFAULT_EPOCHS,
    lr: float = DEFAULT_LR,
    seed: int = DEFAULT_SEED,
    percentile: float = DEFAULT_PERCENTILE,
) -> Dict:
    """
    TODO 3b: Benchmark the full pipeline on one device.
    Returns timing dict.
    """
    # ── TODO 3b START ──
    print(f"    ⚠ TODO 3b: benchmarkOneConfig() is not yet implemented for {device}")
    return {
        "device": str(device),
        "trainSec": 0.0,
        "inferenceSec": 0.0,
        "totalSec": 0.0,
        "threshold": 0.0,
    }
    # ── TODO 3b END ──


# ────────────────────────────────────────────────────────────────────
# TODO 3c: computeSpeedups() — Compare CPU vs GPU timing
#
# Write a function computeSpeedups(cpuResults, gpuResults) that:
#   1. Takes a list of CPU timing dicts and a list of GPU timing dicts
#      (one per run, from multiple benchmarkOneConfig calls)
#   2. Computes mean time per phase for each device
#   3. Returns a dict of speedups: cpuMean / gpuMean for each phase
#      Keys: {"trainSpeedup", "inferenceSpeedup", "totalSpeedup"}
#   4. Speedup > 1 means GPU is faster; < 1 means CPU is faster
# ────────────────────────────────────────────────────────────────────
def computeSpeedups(cpuResults: List[Dict], gpuResults: List[Dict]) -> Dict:
    """
    TODO 3c: Compute per-phase speedups (CPU mean / GPU mean).
    """
    # ── TODO 3c START ──
    print("    ⚠ TODO 3c: computeSpeedups() is not yet implemented")
    return {"trainSpeedup": 0.0, "inferenceSpeedup": 0.0, "totalSpeedup": 0.0}
    # ── TODO 3c END ──


# ────────────────────────────────────────────────────────────────────
# TODO 3d: generateReport() — Human-readable text report
#
# Write a function generateReport(cpuResults, gpuResults, speedups,
#                                  batchSweep=None) that:
#   1. Produces a formatted text report string
#   2. Includes a table of per-run timings for CPU and GPU
#   3. Includes mean ± std for each phase
#   4. Includes the speedup summary
#   5. If batchSweep is provided (list of dicts from batch size sweep),
#      includes a batch size vs timing table
#   6. Returns the report as a string
# ────────────────────────────────────────────────────────────────────
def generateReport(
    cpuResults: List[Dict],
    gpuResults: List[Dict],
    speedups: Dict,
    batchSweep: Optional[List[Dict]] = None,
) -> str:
    """
    TODO 3d: Generate a human-readable benchmark report.
    Returns report string.
    """
    # ── TODO 3d START ──
    lines = ["PumpWatch GPU Benchmark Report", "=" * 40, ""]
    lines.append("⚠ TODO 3d: generateReport() is not yet implemented")
    lines.append("")
    return "\n".join(lines)
    # ── TODO 3d END ──


# ────────────────────────────────────────────────────────────────────
# TODO 3e: main() — Orchestrate the benchmark
#
# Write the main() function that:
#   1. Parses arguments (--csv, --runs, --batchSizes, --outputDir, --seed)
#   2. Loads and prepares data using prepareData()
#   3. Runs benchmarkOneConfig() N times on CPU
#   4. If CUDA is available, runs benchmarkOneConfig() N times on GPU
#   5. Computes speedups with computeSpeedups()
#   6. If --batchSizes is provided (e.g., "32,64,128,256,512,1024"),
#      runs a sweep: for each batch size, run once on CPU and once on GPU
#   7. Generates report with generateReport()
#   8. Saves benchmarkResults.json and benchmarkReport.txt to --outputDir
#
# IMPORTANT: Reset seeds before EACH run (CPU and GPU) so results
# are comparable. The model training is stochastic — different seeds
# produce different models with different timing characteristics.
# ────────────────────────────────────────────────────────────────────


# ── Incomplete TODOs tracker ────────────────────────────────────────
INCOMPLETE_TODOS = []


def checkTodos():
    """Check which TODOs are still stubs and report."""
    todoChecks = [
        ("3a", "timePhase", timePhase),
        ("3b", "benchmarkOneConfig", benchmarkOneConfig),
        ("3c", "computeSpeedups", computeSpeedups),
        ("3d", "generateReport", generateReport),
    ]
    for todoId, name, func in todoChecks:
        import inspect
        source = inspect.getsource(func)
        if f"TODO {todoId}" in source and "not yet implemented" in source:
            INCOMPLETE_TODOS.append(f"TODO {todoId}: {name}()")


def main() -> None:
    # Check for incomplete TODOs
    checkTodos()

    # If main() itself is not implemented, just show the TODO banner
    # ── TODO 3e START ──
    INCOMPLETE_TODOS.append("TODO 3e: main()")
    # ── TODO 3e END ──

    if INCOMPLETE_TODOS:
        print()
        print("=" * 60)
        print("  PumpWatch GPU Benchmark — TODO Status")
        print("=" * 60)
        for todo in INCOMPLETE_TODOS:
            print(f"  ⚠  {todo}")
        print()
        print("  Complete the TODOs above to run the benchmark.")
        print("  See the comments in this file for detailed instructions.")
        print("=" * 60)
        print()
        sys.exit(0)


if __name__ == "__main__":
    main()
